name: Local Config
version: 1.0.0
schema: v1
models:
  - name: lms/llama-3-8b-instruct-1048k
    provider: lmstudio
    model: llama-3-8b-instruct-1048k
    apiBase: http://localhost:12345/v1
    defaultCompletionOptions:
      contextLength: 1048576
  - name: lms/qwen3-coder-30b-a3b-instruct-mlx
    provider: lmstudio
    model: qwen3-coder-30b-a3b-instruct-mlx
    apiBase: http://localhost:12345/v1
  - name: Autodetect
    provider: ollama
    model: AUTODETECT
mcpServers:
  - name: Seqential Thinking
    command: npx
    args:
      - -y
      - "@modelcontextprotocol/server-sequential-thinking"
  - name: Memory
    command: npx
    args:
      - -y
      - "@modelcontextprotocol/server-memory"
  - name: Context7
    command: npx
    args:
      - -y
      - "@upstash/context7-mcp"
  - name: Git
    command: uvx
    args:
      - "mcp-server-git"
